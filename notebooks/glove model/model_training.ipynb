{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../..\")\n",
    "os.chdir(r\"src\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from GloVe.glove_functs import *\n",
    "from GloVe.weights import *\n",
    "\n",
    "from dask import dataframe as dd\n",
    "from scipy import sparse\n",
    "from mittens import Mittens\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cooccurrence matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(14) :\n",
    "\n",
    "    df = dd.read_csv('data/FinalDataframes/FilteredFinalDataFrame_201'+str(i)+'.csv', dtype={'url': 'object'})\n",
    "    print('df ouvert'+str(i))\n",
    "    df = df.compute()\n",
    "    print('df computed'+str(i))\n",
    "    df['text'] = df['text'].map(ast.literal_eval)\n",
    "    df['agenda'] = df['agenda'].map(ast.literal_eval)\n",
    "    print('df mapped'+str(i))\n",
    "    \n",
    "    print('debut après imports'+str(i))\n",
    "    \n",
    "    vocab, word2idx = vocab_dic('data/words/Finalwords_201'+str(i)+'.json')\n",
    "    \n",
    "    print('après vocab_dic'+str(i))\n",
    "    \n",
    "    with open('data/vocabs/vocab_201'+str(i)+'.json', 'w') as f:\n",
    "        json.dump(vocab, f)\n",
    "    \n",
    "    with open('data/word2idx/word2idx_201'+str(i)+'.json', 'w') as f:\n",
    "        json.dump(word2idx, f)\n",
    "    \n",
    "    items = [(j,t) for j,t in enumerate(df['text'])]\n",
    "\n",
    "    coocc = inter_coocc(items, word2idx)\n",
    "    \n",
    "    coocc = coocc.tocsr()\n",
    "    \n",
    "    print('dok_array'+str(i))\n",
    "    \n",
    "    sparse.save_npz('data/glove_cooccurences/glove_cooccurences_201'+str(i)+'.npz', coocc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GloVe Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_embedding = glove2dict('data/glove.6B/glove.6B.50d.txt')\n",
    "\n",
    "for i in range(14) :\n",
    "\n",
    "    with open('vocab_201'+str(i)+'.json') as f:\n",
    "        vocab = json.load(f)\n",
    "        \n",
    "    cooccurrence = sparse.load_npz('data/glove_cooccurences/glove_cooccurrence_201'+str(i)+'.npz')\n",
    "    cooccurrence = cooccurrence.toarray()\n",
    "    \n",
    "    mittens_model = Mittens(n=50, max_iter=1000)\n",
    "    \n",
    "    new_embeddings = mittens_model.fit(\n",
    "        cooccurrence,\n",
    "        vocab=vocab,\n",
    "        initial_embedding_dict= original_embedding)\n",
    "        \n",
    "    a = np.array(vocab)\n",
    "    b =new_embeddings\n",
    "    c = np.column_stack((a, b))\n",
    "    np.savetxt('data/embeddings/embeddings_201'+str(i)+'.txt', c, fmt='%s')\n",
    "    \n",
    "    ################################################################\n",
    "    \n",
    "    original_embedding = glove2dict('data/embeddings/embeddings_201'+str(i)+'.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Document embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(14):\n",
    "    # Path to the GloVe file for a specific year, converted to Word2Vec format\n",
    "    #glove_file_words = datapath('data/embeddings/embeddings_201'+str(i)+'.txt') #This is the old way of doing it.\n",
    "    word2vec_glove_file_words = get_tmpfile(\"format_word2vec.text\")\n",
    "    \n",
    "    # Convert GloVe format to Word2Vec format\n",
    "    glove2word2vec('data/embeddings/embeddings_201'+str(i)+'.txt', word2vec_glove_file_words)\n",
    "    \n",
    "    # Load the Word2Vec model\n",
    "    model = KeyedVectors.load_word2vec_format(word2vec_glove_file_words)\n",
    "    \n",
    "    # Load the words for which weights need to be computed\n",
    "    with open('data/words/Finalwords_201'+str(i)+'.json') as f:\n",
    "        words = json.load(f)\n",
    "    \n",
    "    # Compute weights for the words\n",
    "    weights = get_weights_word2vec(words, a=1e-3)\n",
    "    \n",
    "    # Open the corpus for the year and reset its index\n",
    "    df = standard_opening('data/FinalDataframes/FilteredFinalDataFrame_201'+str(i)+'.csv', True)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    # Calculate sentence embeddings for each text in the dataframe\n",
    "    df['sentence_embedding'] = df['text'].apply(get_sentence_embeddings, weights=weights, model=model)\n",
    "    \n",
    "    # Filter and organize dataframe columns\n",
    "    df = df[['text', 'sentence_embedding', 'source', 'party', 'keywords', 'Speaker']]\n",
    "    df = df[df['sentence_embedding'].notna()]\n",
    "    \n",
    "    # Remove the principal component from the sentence embeddings\n",
    "    b = np.array(df['sentence_embedding'].tolist())\n",
    "    b = remove_pc(b, npc=1)\n",
    "    b = b.astype(str)\n",
    "    df['sentence_embedding'] = b.tolist()\n",
    "    \n",
    "    # Save the dataframe with sentence embeddings to a CSV file\n",
    "    df.to_csv('data/sentence_embeddings/sentence_embeddings_201'+str(i)+'.csv', index=True)\n",
    "    \n",
    "    # Also save the text and processed embeddings to a TXT file\n",
    "    c = np.column_stack((df['text'].apply(phrase), b))\n",
    "    np.savetxt('data/sentence_embeddings/sentence_embeddings_201'+str(i)+'.txt', c, fmt='%s')\n",
    "\n",
    "    print(f'end of loop {i}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
