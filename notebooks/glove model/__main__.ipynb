{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../..\")\n",
    "os.chdir(r\"src\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from GloVe.glove_functs import *\n",
    "from GloVe.weights import *\n",
    "\n",
    "from dask import dataframe as dd\n",
    "from scipy import sparse\n",
    "from mittens import Mittens\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from datetime import datetime\n",
    "import os, psutil\n",
    "\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cooccurrence matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(14) :\n",
    "\n",
    "    df = dd.read_csv('data/without parliament/FinalDataframes/FilteredFinalDataFrame_201'+str(i)+'_WP.csv', dtype={'url': 'object'})\n",
    "    print('df ouvert'+str(i))\n",
    "    df = df.compute()\n",
    "    print('df computed'+str(i))\n",
    "    df['text'] = df['text'].map(ast.literal_eval)\n",
    "    df['agenda'] = df['agenda'].map(ast.literal_eval)\n",
    "    print('df mapped'+str(i))\n",
    "    \n",
    "    print('debut après imports'+str(i))\n",
    "    \n",
    "    vocab, word2idx = vocab_dic('data/without parliament/words/Finalwords_201'+str(i)+'_WP.json')\n",
    "    \n",
    "    print('après vocab_dic'+str(i))\n",
    "    \n",
    "    with open('data/without parliament/vocabs/vocab_201'+str(i)+'_WP.json', 'w') as f:\n",
    "        json.dump(vocab, f)\n",
    "    \n",
    "    with open('data/without parliament/word2idx/word2idx_201'+str(i)+'_WP.json', 'w') as f:\n",
    "        json.dump(word2idx, f)\n",
    "    \n",
    "    items = [(j,t) for j,t in enumerate(df['text'])]\n",
    "\n",
    "    coocc = inter_coocc(items, word2idx)\n",
    "    \n",
    "    coocc = coocc.tocsr()\n",
    "    \n",
    "    print('dok_array'+str(i))\n",
    "    \n",
    "    sparse.save_npz('data//without parliament/glove_cooccurences/glove_cooccurences_201'+str(i)+'_WP.npz', coocc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GloVe Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parallel_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_embedding = glove2dict('data/glove.6B/glove.6B.50d.txt')\n",
    "\n",
    "for i in range(14) :\n",
    "\n",
    "    print(i)\n",
    "\n",
    "    with open('data/without parliament/vocabs/vocab_201'+str(i)+'_WP.json') as f:\n",
    "        vocab = json.load(f)\n",
    "        \n",
    "    cooccurrence = sparse.load_npz('data//without parliament/glove_cooccurences/glove_cooccurences_201'+str(i)+'_WP.npz')\n",
    "    cooccurrence = cooccurrence.toarray()\n",
    "    \n",
    "    mittens_model = Mittens(n=50, max_iter=1000)\n",
    "    \n",
    "    new_embeddings = mittens_model.fit(\n",
    "        cooccurrence,\n",
    "        vocab=vocab,\n",
    "        initial_embedding_dict= original_embedding)\n",
    "        \n",
    "    a = np.array(vocab)\n",
    "    b =new_embeddings\n",
    "    c = np.column_stack((a, b))\n",
    "    np.savetxt('data/without parliament/embeddings/embeddings_201'+str(i)+'_WP.txt', c, fmt='%s')\n",
    "    \n",
    "    ################################################################\n",
    "    \n",
    "    original_embedding = glove2dict('data/without parliament/embeddings/embeddings_201'+str(i)+'_WP.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Document embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing principal component...\n",
      "Computing principal components...\n",
      "end of loop 0\n",
      "Removing principal component...\n",
      "Computing principal components...\n",
      "end of loop 1\n",
      "Removing principal component...\n",
      "Computing principal components...\n",
      "end of loop 2\n",
      "Removing principal component...\n",
      "Computing principal components...\n",
      "end of loop 3\n",
      "Removing principal component...\n",
      "Computing principal components...\n",
      "end of loop 4\n",
      "Removing principal component...\n",
      "Computing principal components...\n",
      "end of loop 5\n",
      "Removing principal component...\n",
      "Computing principal components...\n",
      "end of loop 6\n",
      "Removing principal component...\n",
      "Computing principal components...\n",
      "end of loop 7\n",
      "Removing principal component...\n",
      "Computing principal components...\n",
      "end of loop 8\n",
      "Removing principal component...\n",
      "Computing principal components...\n",
      "end of loop 9\n",
      "Removing principal component...\n",
      "Computing principal components...\n",
      "end of loop 10\n",
      "Removing principal component...\n",
      "Computing principal components...\n",
      "end of loop 11\n",
      "Removing principal component...\n",
      "Computing principal components...\n",
      "end of loop 12\n",
      "Removing principal component...\n",
      "Computing principal components...\n",
      "end of loop 13\n"
     ]
    }
   ],
   "source": [
    "for i in range(14):\n",
    "    # Path to the GloVe file for a specific year, converted to Word2Vec format\n",
    "    #glove_file_words = datapath('data/embeddings/embeddings_201'+str(i)+'.txt') #This is the old way of doing it.\n",
    "    word2vec_glove_file_words = get_tmpfile(\"format_word2vec.text\")\n",
    "    \n",
    "    # Convert GloVe format to Word2Vec format\n",
    "    glove2word2vec('data/without parliament/embeddings/embeddings_201'+str(i)+'_WP.txt', word2vec_glove_file_words)\n",
    "    \n",
    "    # Load the Word2Vec model\n",
    "    model = KeyedVectors.load_word2vec_format(word2vec_glove_file_words)\n",
    "    \n",
    "    # Load the words for which weights need to be computed\n",
    "    with open('data/without parliament/words/Finalwords_201'+str(i)+'_WP.json') as f:\n",
    "        words = json.load(f)\n",
    "    \n",
    "    # Compute weights for the words\n",
    "    weights = get_weights_word2vec(words, a=1e-3)\n",
    "    \n",
    "    # Open the corpus for the year and reset its index\n",
    "    df = standard_opening('data/without parliament/FinalDataframes/FilteredFinalDataFrame_201'+str(i)+'_WP.csv', True)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    # Calculate sentence embeddings for each text in the dataframe\n",
    "    df['sentence_embedding'] = df['text'].apply(get_sentence_embeddings, weights=weights, model=model)\n",
    "    \n",
    "    # Filter and organize dataframe columns\n",
    "    df['party'] = 0\n",
    "    df['Speaker'] = 0\n",
    "    df = df[['text', 'sentence_embedding', 'source', 'party', 'keywords', 'Speaker']]\n",
    "    df = df[df['sentence_embedding'].notna()]\n",
    "    \n",
    "    # Remove the principal component from the sentence embeddings\n",
    "    b = np.array(df['sentence_embedding'].tolist())\n",
    "    b = remove_pc(b, npc=1)\n",
    "    b = b.astype(str)\n",
    "    df['sentence_embedding'] = b.tolist()\n",
    "    \n",
    "    # Save the dataframe with sentence embeddings to a CSV file\n",
    "    df.to_csv('data/without parliament/sentence_embeddings/sentence_embeddings_201'+str(i)+'.csv', index=True)\n",
    "    \n",
    "    # Also save the text and processed embeddings to a TXT file\n",
    "    c = np.column_stack((df['text'].apply(phrase), b))\n",
    "    np.savetxt('data/without parliament/sentence_embeddings/sentence_embeddings_201'+str(i)+'.txt', c, fmt='%s')\n",
    "\n",
    "    print(f'end of loop {i}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
