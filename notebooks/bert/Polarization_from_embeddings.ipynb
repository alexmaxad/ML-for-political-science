{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from engine import *\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "os.chdir(\"../..\")\n",
    "os.chdir(r\"src\")\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from GloVe.weights import *\n",
    "#from Axes.models import *\n",
    "from Axes.projection_functions import *\n",
    "from Axes.axes_definition import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data formating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 7 # Donc en 2017\n",
    "df = standard_opening(\n",
    "            \"data/FinalDataframes/FilteredFinalDataFrame_201\" + str(i) + \".csv\", True\n",
    "        )\n",
    "df = df[df[\"source\"] == \"par\"]\n",
    "\n",
    "df_cos = pd.read_csv('data/current_dataframes/df')\n",
    "df_cos = df_cos[df_cos['year'] == 2010+i]\n",
    "df_cos = df_cos[df_cos[\"source\"] == \"par\"].reset_index()\n",
    "\n",
    "df['cos axe'] = list(df_cos['cos axe 1'])\n",
    "df = df.loc[df['party'].isin(['Lab', 'Con'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'notebooks/embeddings polarization/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Polarized corpus selection with GloVe embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantiles(data, percentiles):\n",
    "    \"\"\"\n",
    "    Get quantiles from a distribution.\n",
    "    \n",
    "    Parameters:\n",
    "        data (array-like): The data.\n",
    "        percentiles (array-like): The percentiles to compute (0-100).\n",
    "    \n",
    "    Returns:\n",
    "        quantiles (array): The values at the specified percentiles.\n",
    "    \"\"\"\n",
    "    return np.percentile(data, percentiles)\n",
    "\n",
    "percentiles = [25, 75]\n",
    "quantiles = get_quantiles(df['cos axe'], percentiles)\n",
    "\n",
    "df = df.loc[(df['cos axe'] < quantiles[0]) | (df['cos axe'] > quantiles[1])]\n",
    "#df = df.loc[(df['cos axe'] < quantiles[0])]\n",
    "\n",
    "df.to_csv('df_news.csv')\n",
    "\n",
    "data = df['text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict_corpus(data_words):\n",
    "    import gensim.corpora as corpora\n",
    "    \n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(data_words)\n",
    "\n",
    "    # Create Corpus\n",
    "    texts = data_words\n",
    "\n",
    "    # Term Document Frequency\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "    return corpus, id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_lda(data):\n",
    "    \n",
    "    corpus, id2word = create_dict_corpus(data)\n",
    "\n",
    "    return data, corpus, id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_processed_lda, corpus_lda, id2word_lda = preprocessing_lda(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(texts_processed_lda, open('texts_processed_lda.pkl', 'wb'))\n",
    "pickle.dump((corpus_lda, id2word_lda), open('corpus_lda.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = pickle.load(open('texts_processed_lda.pkl', 'rb'))\n",
    "corpus, id2word = pickle.load(open('corpus_lda.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lda_model(corpus, id2word, texts, model_type='lda', start=10, limit=50, step=3):\n",
    "    import os\n",
    "    import gensim\n",
    "    import numpy as np\n",
    "    from gensim.models import CoherenceModel\n",
    "    \n",
    "    if model_type == 'mallet' and not os.path.exists('mallet-2.0.8.zip'):\n",
    "        os.system('wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip')\n",
    "        os.system('unzip mallet-2.0.8.zip')\n",
    "    mallet_path = 'mallet-2.0.8/bin/mallet'\n",
    "    \n",
    "    def lda_model_func(corpus, id2word, num_topics):\n",
    "        print(f'Training model with {num_topics} topics')\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics, \n",
    "                                           random_state=42,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "        coherence_model = CoherenceModel(model=model, texts=texts, dictionary=id2word, coherence='c_v')\n",
    "        coh_v = coherence_model.get_coherence()\n",
    "        print(f'num of topic: {num_topics}/{limit}, coherence value: {coh_v}')\n",
    "        return model, coh_v\n",
    "    \n",
    "    \n",
    "    def mallet_model_func(mallet_path, corpus, num_topics, id2word):\n",
    "        print(f'Training model with {num_topics} topics')\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, \n",
    "                                                     corpus=corpus, \n",
    "                                                     num_topics=num_topics, \n",
    "                                                     id2word=id2word, \n",
    "                                                     iterations=50,\n",
    "                                                     workers=1,\n",
    "                                                     random_seed=42)\n",
    "        coherence_model = CoherenceModel(model=model, texts=texts, dictionary=id2word, coherence='c_v')\n",
    "        coh_v = coherence_model.get_coherence()\n",
    "        print(f'num of topic: {num_topics}/{limit}, coherence value: {coh_v}')\n",
    "        return model, coh_v\n",
    "    \n",
    "    results = []\n",
    "    for num_topics in range(start, limit+1, step):\n",
    "        if model_type == 'lda':\n",
    "            thread = lda_model_func(corpus, id2word, num_topics)\n",
    "        else:\n",
    "            thread = mallet_model_func(mallet_path, corpus, num_topics, id2word)\n",
    "        results.append(thread)\n",
    "    \n",
    "    coherence_values_dict = {}\n",
    "    coherence_values = []\n",
    "    model_dict = {}\n",
    "    \n",
    "    for i in range(len(results)):\n",
    "        model, coh_v = results[i]\n",
    "        model_dict[start+i] = model\n",
    "        coherence_values_dict[start+i] = coh_v\n",
    "        coherence_values.append(coh_v)\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(range(start, limit+1, step), coherence_values, marker='o')\n",
    "    plt.xlabel('n_topics')\n",
    "    plt.ylabel('coherence score')\n",
    "    plt.savefig(f'{model_type}_coherence_values_topics.png')\n",
    "    plt.close()\n",
    "    \n",
    "    max_idx = np.argmax(coherence_values)\n",
    "    \n",
    "    return model_dict, coherence_values_dict, max_idx+start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with 31 topics\n",
      "num of topic: 31/31, coherence value: 0.41768458219973087\n"
     ]
    }
   ],
   "source": [
    "lda_models, coh_values, max_idx = train_lda_model(corpus, id2word, texts, model_type='lda', start=31, limit=31, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_topics</th>\n",
       "      <th>coh_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31</td>\n",
       "      <td>0.417685</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_topics   coh_val\n",
       "0        31  0.417685"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ntopics_coh = pd.DataFrame(list(coh_values.items()), columns=['n_topics', 'coh_val'])\n",
    "df_ntopics_coh.sort_values(by='coh_val', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the model with K=?\n",
    "K = 31\n",
    "lda_model = lda_models[K]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [('peopl', 0.07057808),\n",
       "   ('mani', 0.035128433),\n",
       "   ('time', 0.029461054),\n",
       "   ('know', 0.02717885),\n",
       "   ('want', 0.025428988),\n",
       "   ('look', 0.023120673),\n",
       "   ('come', 0.019300655),\n",
       "   ('good', 0.017782414),\n",
       "   ('like', 0.017775413),\n",
       "   ('thing', 0.017262198)]),\n",
       " (1,\n",
       "  [('resid', 0.4169618),\n",
       "   ('town', 0.24864845),\n",
       "   ('scrap', 0.04617897),\n",
       "   ('academi', 0.029542683),\n",
       "   ('rubbish', 0.017725876),\n",
       "   ('broker', 0.007829354),\n",
       "   ('yemeni', 0.007598497),\n",
       "   ('ceasefir', 0.001276323),\n",
       "   ('constitu', 6.4554035e-05),\n",
       "   ('mileag', 1.4859683e-05)]),\n",
       " (2,\n",
       "  [('budget', 0.08824636),\n",
       "   ('labour', 0.07812836),\n",
       "   ('data', 0.0564751),\n",
       "   ('public', 0.047576945),\n",
       "   ('employ', 0.046441298),\n",
       "   ('worker', 0.040704567),\n",
       "   ('spend', 0.03797215),\n",
       "   ('custom', 0.035052318),\n",
       "   ('rise', 0.032739244),\n",
       "   ('claim', 0.03033037)]),\n",
       " (3,\n",
       "  [('polic', 0.17411944),\n",
       "   ('victim', 0.13803579),\n",
       "   ('justic', 0.13783965),\n",
       "   ('crime', 0.077537864),\n",
       "   ('crimin', 0.066206194),\n",
       "   ('case', 0.04848691),\n",
       "   ('offic', 0.041317176),\n",
       "   ('harass', 0.026696425),\n",
       "   ('legal', 0.0260639),\n",
       "   ('claimant', 0.022225253)]),\n",
       " (4,\n",
       "  [('unfair', 0.23876412),\n",
       "   ('civil', 0.1556406),\n",
       "   ('servant', 0.07445409),\n",
       "   ('terrorist', 0.07269681),\n",
       "   ('correctli', 0.054201752),\n",
       "   ('legaci', 0.04703787),\n",
       "   ('forgotten', 0.040141713),\n",
       "   ('soldier', 0.03060015),\n",
       "   ('atroc', 0.02772567),\n",
       "   ('stormont', 0.008038913)]),\n",
       " (5,\n",
       "  [('would', 0.060430657),\n",
       "   ('right', 0.055660404),\n",
       "   ('make', 0.023488848),\n",
       "   ('point', 0.021643717),\n",
       "   ('deal', 0.013837618),\n",
       "   ('chang', 0.013815237),\n",
       "   ('give', 0.013298991),\n",
       "   ('agre', 0.013187929),\n",
       "   ('could', 0.013180292),\n",
       "   ('whether', 0.012498641)]),\n",
       " (6,\n",
       "  [('agreement', 0.18147752),\n",
       "   ('trade', 0.14070943),\n",
       "   ('compani', 0.13118294),\n",
       "   ('market', 0.09168628),\n",
       "   ('intern', 0.048205886),\n",
       "   ('singl', 0.047733232),\n",
       "   ('free', 0.046662595),\n",
       "   ('sell', 0.040031817),\n",
       "   ('phase', 0.028236257),\n",
       "   ('deal', 0.019731283)]),\n",
       " (7,\n",
       "  [('power', 0.24635395),\n",
       "   ('legisl', 0.19961882),\n",
       "   ('devolv', 0.052153245),\n",
       "   ('administr', 0.04739731),\n",
       "   ('provis', 0.045479376),\n",
       "   ('principl', 0.040570024),\n",
       "   ('scrutini', 0.040220574),\n",
       "   ('draft', 0.032285),\n",
       "   ('primari', 0.027667578),\n",
       "   ('secondari', 0.022960499)]),\n",
       " (8,\n",
       "  [('food', 0.2713612),\n",
       "   ('human', 0.1881204),\n",
       "   ('transit', 0.11952268),\n",
       "   ('biggest', 0.08287465),\n",
       "   ('deleg', 0.06789281),\n",
       "   ('water', 0.059726443),\n",
       "   ('legislatur', 0.02561009),\n",
       "   ('thursday', 0.020409454),\n",
       "   ('zone', 0.015958076),\n",
       "   ('tuesday', 0.012146765)]),\n",
       " (9,\n",
       "  [('prison', 0.19606295),\n",
       "   ('post', 0.14650112),\n",
       "   ('offic', 0.14232711),\n",
       "   ('justic', 0.057591487),\n",
       "   ('convict', 0.047943126),\n",
       "   ('ministri', 0.04674756),\n",
       "   ('interpret', 0.040693678),\n",
       "   ('knew', 0.033951852),\n",
       "   ('custodi', 0.031439915),\n",
       "   ('suicid', 0.030164381)]),\n",
       " (10,\n",
       "  [('differ', 0.090420075),\n",
       "   ('find', 0.06689741),\n",
       "   ('learn', 0.0652923),\n",
       "   ('listen', 0.04360898),\n",
       "   ('trust', 0.040588625),\n",
       "   ('difficult', 0.036933657),\n",
       "   ('noth', 0.033514697),\n",
       "   ('select', 0.027630502),\n",
       "   ('kind', 0.026143232),\n",
       "   ('wrong', 0.026017383)]),\n",
       " (11,\n",
       "  [('review', 0.09168866),\n",
       "   ('publish', 0.083648),\n",
       "   ('consult', 0.08335476),\n",
       "   ('inform', 0.081165306),\n",
       "   ('commiss', 0.07413825),\n",
       "   ('evid', 0.062788785),\n",
       "   ('assess', 0.056696102),\n",
       "   ('propos', 0.046932),\n",
       "   ('impact', 0.032203343),\n",
       "   ('advic', 0.029929223)]),\n",
       " (12,\n",
       "  [('children', 0.42275214),\n",
       "   ('school', 0.25557894),\n",
       "   ('obvious', 0.08412701),\n",
       "   ('educ', 0.06046761),\n",
       "   ('free', 0.022197872),\n",
       "   ('consciou', 0.020739907),\n",
       "   ('forum', 0.012291235),\n",
       "   ('primari', 0.011047232),\n",
       "   ('secondari', 0.007148537),\n",
       "   ('outstand', 0.005066992)]),\n",
       " (13,\n",
       "  [('univers', 0.13877404),\n",
       "   ('educ', 0.079330616),\n",
       "   ('skill', 0.07377499),\n",
       "   ('innov', 0.06359135),\n",
       "   ('statu', 0.04727042),\n",
       "   ('scienc', 0.04627659),\n",
       "   ('student', 0.043152396),\n",
       "   ('apprenticeship', 0.033394314),\n",
       "   ('career', 0.03194444),\n",
       "   ('colleg', 0.031819876)]),\n",
       " (14,\n",
       "  [('offenc', 0.14791974),\n",
       "   ('sentenc', 0.14246781),\n",
       "   ('enforc', 0.093443416),\n",
       "   ('death', 0.071623854),\n",
       "   ('seriou', 0.06475906),\n",
       "   ('penalti', 0.06329244),\n",
       "   ('maximum', 0.056734495),\n",
       "   ('prosecut', 0.05271499),\n",
       "   ('night', 0.047167476),\n",
       "   ('caus', 0.036623724)]),\n",
       " (15,\n",
       "  [('work', 0.045893118),\n",
       "   ('constitu', 0.027170638),\n",
       "   ('support', 0.0247249),\n",
       "   ('also', 0.018863741),\n",
       "   ('nation', 0.017679341),\n",
       "   ('across', 0.016527241),\n",
       "   ('import', 0.01586932),\n",
       "   ('welcom', 0.0146491015),\n",
       "   ('futur', 0.014472131),\n",
       "   ('opportun', 0.013757487)]),\n",
       " (16,\n",
       "  [('provid', 0.047422715),\n",
       "   ('ensur', 0.037519537),\n",
       "   ('need', 0.027099298),\n",
       "   ('includ', 0.025870806),\n",
       "   ('plan', 0.024494315),\n",
       "   ('current', 0.02005858),\n",
       "   ('access', 0.019483631),\n",
       "   ('requir', 0.019243415),\n",
       "   ('oper', 0.019235555),\n",
       "   ('group', 0.016907483)]),\n",
       " (17,\n",
       "  [('inquiri', 0.111924),\n",
       "   ('report', 0.073885255),\n",
       "   ('transpar', 0.07383954),\n",
       "   ('investig', 0.05903252),\n",
       "   ('recommend', 0.05163448),\n",
       "   ('independ', 0.044720653),\n",
       "   ('conduct', 0.04081468),\n",
       "   ('tragedi', 0.031862833),\n",
       "   ('code', 0.031366237),\n",
       "   ('public', 0.030895373)]),\n",
       " (18,\n",
       "  [('industri', 0.08793788),\n",
       "   ('invest', 0.08355847),\n",
       "   ('sector', 0.07967649),\n",
       "   ('economi', 0.05374773),\n",
       "   ('product', 0.052954882),\n",
       "   ('strategi', 0.037297435),\n",
       "   ('busi', 0.03434465),\n",
       "   ('econom', 0.02628996),\n",
       "   ('infrastructur', 0.024060646),\n",
       "   ('growth', 0.023785092)]),\n",
       " (19,\n",
       "  [('women', 0.23391391),\n",
       "   ('northern', 0.08895754),\n",
       "   ('abus', 0.08770217),\n",
       "   ('ireland', 0.0780431),\n",
       "   ('domest', 0.07327057),\n",
       "   ('poverti', 0.03945463),\n",
       "   ('violenc', 0.029431842),\n",
       "   ('household', 0.028511707),\n",
       "   ('afternoon', 0.026364794),\n",
       "   ('feed', 0.024353918)]),\n",
       " (20,\n",
       "  [('european', 0.19128753),\n",
       "   ('leav', 0.15127714),\n",
       "   ('union', 0.13947994),\n",
       "   ('unit', 0.123238645),\n",
       "   ('kingdom', 0.06177291),\n",
       "   ('relationship', 0.03972045),\n",
       "   ('democrat', 0.027851647),\n",
       "   ('presid', 0.020712433),\n",
       "   ('membership', 0.018260771),\n",
       "   ('channel', 0.016063385)]),\n",
       " (21,\n",
       "  [('issu', 0.03532768),\n",
       "   ('made', 0.025207087),\n",
       "   ('take', 0.02137619),\n",
       "   ('rais', 0.021013144),\n",
       "   ('debat', 0.019931845),\n",
       "   ('clear', 0.019324418),\n",
       "   ('respons', 0.01900927),\n",
       "   ('concern', 0.01899847),\n",
       "   ('matter', 0.014926214),\n",
       "   ('process', 0.01386732)]),\n",
       " (22,\n",
       "  [('wale', 0.3216917),\n",
       "   ('chancellor', 0.15365414),\n",
       "   ('welsh', 0.10873429),\n",
       "   ('harm', 0.07112325),\n",
       "   ('boost', 0.050969847),\n",
       "   ('cardiff', 0.047867686),\n",
       "   ('hmrc', 0.038820017),\n",
       "   ('fuel', 0.02158001),\n",
       "   ('exchequ', 0.020030096),\n",
       "   ('grand', 0.01848035)]),\n",
       " (23,\n",
       "  [('role', 0.27972576),\n",
       "   ('play', 0.2707378),\n",
       "   ('march', 0.0750488),\n",
       "   ('peac', 0.061431374),\n",
       "   ('exempt', 0.04690728),\n",
       "   ('round', 0.04287127),\n",
       "   ('prosper', 0.031512924),\n",
       "   ('websit', 0.031296138),\n",
       "   ('australia', 0.01340967),\n",
       "   ('region', 0.011539645)]),\n",
       " (24,\n",
       "  [('foreign', 0.25716323),\n",
       "   ('leed', 0.15449537),\n",
       "   ('soon', 0.12585828),\n",
       "   ('franc', 0.052010264),\n",
       "   ('abroad', 0.050775655),\n",
       "   ('accus', 0.029463554),\n",
       "   ('saudi', 0.028471842),\n",
       "   ('justif', 0.019304598),\n",
       "   ('arabia', 0.018908825),\n",
       "   ('spokesman', 0.014769753)]),\n",
       " (25,\n",
       "  [('protect', 0.18599358),\n",
       "   ('regul', 0.16208427),\n",
       "   ('standard', 0.13288863),\n",
       "   ('environ', 0.081522115),\n",
       "   ('framework', 0.05577186),\n",
       "   ('safeti', 0.05404198),\n",
       "   ('risk', 0.049046244),\n",
       "   ('regulatori', 0.044042163),\n",
       "   ('guarante', 0.034853574),\n",
       "   ('certainti', 0.032557774)]),\n",
       " (26,\n",
       "  [('busi', 0.13014059),\n",
       "   ('cost', 0.10106412),\n",
       "   ('money', 0.09019569),\n",
       "   ('financi', 0.066669784),\n",
       "   ('scheme', 0.065178834),\n",
       "   ('pension', 0.043462295),\n",
       "   ('small', 0.040880017),\n",
       "   ('system', 0.037774086),\n",
       "   ('rate', 0.037485186),\n",
       "   ('properti', 0.026690664)]),\n",
       " (27,\n",
       "  [('forc', 0.3865257),\n",
       "   ('humanitarian', 0.08918552),\n",
       "   ('allparti', 0.06933109),\n",
       "   ('spread', 0.05353482),\n",
       "   ('conflict', 0.050501842),\n",
       "   ('yemen', 0.046012763),\n",
       "   ('intern', 0.029102085),\n",
       "   ('coalit', 0.025210526),\n",
       "   ('legitim', 0.024646947),\n",
       "   ('syria', 0.022235451)]),\n",
       " (28,\n",
       "  [('train', 0.09919783),\n",
       "   ('east', 0.08285161),\n",
       "   ('royal', 0.07551264),\n",
       "   ('instrument', 0.066548064),\n",
       "   ('line', 0.064911835),\n",
       "   ('network', 0.04564529),\n",
       "   ('defenc', 0.03876934),\n",
       "   ('guidelin', 0.035290174),\n",
       "   ('rout', 0.035126917),\n",
       "   ('retail', 0.034852162)]),\n",
       " (29,\n",
       "  [('servic', 0.046690818),\n",
       "   ('famili', 0.02972939),\n",
       "   ('care', 0.02601756),\n",
       "   ('health', 0.025467437),\n",
       "   ('need', 0.025001932),\n",
       "   ('social', 0.022143615),\n",
       "   ('work', 0.019555958),\n",
       "   ('staff', 0.01276199),\n",
       "   ('increas', 0.012703061),\n",
       "   ('support', 0.01255102)]),\n",
       " (30,\n",
       "  [('local', 0.33775976),\n",
       "   ('fund', 0.24508712),\n",
       "   ('author', 0.14416526),\n",
       "   ('commun', 0.09300859),\n",
       "   ('despit', 0.04958328),\n",
       "   ('delay', 0.03505477),\n",
       "   ('area', 0.014419856),\n",
       "   ('dump', 0.008629436),\n",
       "   ('penni', 0.0038076912),\n",
       "   ('extra', 0.0030932282)])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the top-10 topic keywords for each topic and their corresponding probs\n",
    "lda_models[K].show_topics(num_topics=-1, formatted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index each topic\n",
    "topics = {x:y for x,y in lda_model.show_topics(num_topics=-1, num_words=10, formatted=False)}\n",
    "df_topic = []\n",
    "for j in range(len(topics)):\n",
    "    df_topic.append([j, [each[0] for each in topics[j]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = df['party'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank topics by the mass of probabilities\n",
    "def rank_topics(lda_model, corpus):\n",
    "    idx_prob = [[i, 0] for i in range(lda_model.num_topics)]\n",
    "    for idx_doc, rows in enumerate(lda_model[corpus]):\n",
    "        for j, (idx_topic, prob) in enumerate(rows[0]):\n",
    "            idx_prob[idx_topic][1] += prob\n",
    "    idx_prob.sort(key=lambda x:x[1], reverse=True)\n",
    "    return idx_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[15, 1547.008503060788],\n",
       " [5, 1461.5668585933745],\n",
       " [0, 1287.3503456730396],\n",
       " [21, 1270.911610128358],\n",
       " [29, 782.6186088277027],\n",
       " [16, 709.3686178131029],\n",
       " [18, 250.10670069698244],\n",
       " [10, 212.9987901384011],\n",
       " [2, 172.21862554736435],\n",
       " [11, 155.65554065816104],\n",
       " [26, 153.20812607277185],\n",
       " [20, 102.81011934578419],\n",
       " [7, 74.15158522687852],\n",
       " [13, 71.84708705823869],\n",
       " [30, 68.52157135959715],\n",
       " [28, 59.578360565938056],\n",
       " [25, 56.29575486574322],\n",
       " [6, 55.29404758941382],\n",
       " [19, 47.631963931024075],\n",
       " [3, 43.10020915605128],\n",
       " [17, 35.0522120250389],\n",
       " [12, 29.196615272201598],\n",
       " [9, 28.45920663420111],\n",
       " [23, 27.99658631812781],\n",
       " [14, 21.636693441309035],\n",
       " [8, 19.50822769012302],\n",
       " [22, 19.38922882452607],\n",
       " [27, 18.50985781941563],\n",
       " [24, 9.564348483458161],\n",
       " [1, 6.775415416806936],\n",
       " [4, 5.037258157506585]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_ranks = rank_topics(lda_model, corpus)\n",
    "topic_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_idx</th>\n",
       "      <th>topic_stems</th>\n",
       "      <th>probs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[peopl, mani, time, know, want, look, come, go...</td>\n",
       "      <td>1547.008503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[resid, town, scrap, academi, rubbish, broker,...</td>\n",
       "      <td>1461.566859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[budget, labour, data, public, employ, worker,...</td>\n",
       "      <td>1287.350346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[polic, victim, justic, crime, crimin, case, o...</td>\n",
       "      <td>1270.911610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[unfair, civil, servant, terrorist, correctli,...</td>\n",
       "      <td>782.618609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>[would, right, make, point, deal, chang, give,...</td>\n",
       "      <td>709.368618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>[agreement, trade, compani, market, intern, si...</td>\n",
       "      <td>250.106701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>[power, legisl, devolv, administr, provis, pri...</td>\n",
       "      <td>212.998790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>[food, human, transit, biggest, deleg, water, ...</td>\n",
       "      <td>172.218626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>[prison, post, offic, justic, convict, ministr...</td>\n",
       "      <td>155.655541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>[differ, find, learn, listen, trust, difficult...</td>\n",
       "      <td>153.208126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>[review, publish, consult, inform, commiss, ev...</td>\n",
       "      <td>102.810119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>[children, school, obvious, educ, free, consci...</td>\n",
       "      <td>74.151585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>[univers, educ, skill, innov, statu, scienc, s...</td>\n",
       "      <td>71.847087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>[offenc, sentenc, enforc, death, seriou, penal...</td>\n",
       "      <td>68.521571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>[work, constitu, support, also, nation, across...</td>\n",
       "      <td>59.578361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>[provid, ensur, need, includ, plan, current, a...</td>\n",
       "      <td>56.295755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>[inquiri, report, transpar, investig, recommen...</td>\n",
       "      <td>55.294048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>[industri, invest, sector, economi, product, s...</td>\n",
       "      <td>47.631964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>[women, northern, abus, ireland, domest, pover...</td>\n",
       "      <td>43.100209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>[european, leav, union, unit, kingdom, relatio...</td>\n",
       "      <td>35.052212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>[issu, made, take, rais, debat, clear, respons...</td>\n",
       "      <td>29.196615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>[wale, chancellor, welsh, harm, boost, cardiff...</td>\n",
       "      <td>28.459207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>[role, play, march, peac, exempt, round, prosp...</td>\n",
       "      <td>27.996586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>[foreign, leed, soon, franc, abroad, accus, sa...</td>\n",
       "      <td>21.636693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>[protect, regul, standard, environ, framework,...</td>\n",
       "      <td>19.508228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>[busi, cost, money, financi, scheme, pension, ...</td>\n",
       "      <td>19.389229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>[forc, humanitarian, allparti, spread, conflic...</td>\n",
       "      <td>18.509858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>[train, east, royal, instrument, line, network...</td>\n",
       "      <td>9.564348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>[servic, famili, care, health, need, social, w...</td>\n",
       "      <td>6.775415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>[local, fund, author, commun, despit, delay, a...</td>\n",
       "      <td>5.037258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    topic_idx                                        topic_stems        probs\n",
       "0           0  [peopl, mani, time, know, want, look, come, go...  1547.008503\n",
       "1           1  [resid, town, scrap, academi, rubbish, broker,...  1461.566859\n",
       "2           2  [budget, labour, data, public, employ, worker,...  1287.350346\n",
       "3           3  [polic, victim, justic, crime, crimin, case, o...  1270.911610\n",
       "4           4  [unfair, civil, servant, terrorist, correctli,...   782.618609\n",
       "5           5  [would, right, make, point, deal, chang, give,...   709.368618\n",
       "6           6  [agreement, trade, compani, market, intern, si...   250.106701\n",
       "7           7  [power, legisl, devolv, administr, provis, pri...   212.998790\n",
       "8           8  [food, human, transit, biggest, deleg, water, ...   172.218626\n",
       "9           9  [prison, post, offic, justic, convict, ministr...   155.655541\n",
       "10         10  [differ, find, learn, listen, trust, difficult...   153.208126\n",
       "11         11  [review, publish, consult, inform, commiss, ev...   102.810119\n",
       "12         12  [children, school, obvious, educ, free, consci...    74.151585\n",
       "13         13  [univers, educ, skill, innov, statu, scienc, s...    71.847087\n",
       "14         14  [offenc, sentenc, enforc, death, seriou, penal...    68.521571\n",
       "15         15  [work, constitu, support, also, nation, across...    59.578361\n",
       "16         16  [provid, ensur, need, includ, plan, current, a...    56.295755\n",
       "17         17  [inquiri, report, transpar, investig, recommen...    55.294048\n",
       "18         18  [industri, invest, sector, economi, product, s...    47.631964\n",
       "19         19  [women, northern, abus, ireland, domest, pover...    43.100209\n",
       "20         20  [european, leav, union, unit, kingdom, relatio...    35.052212\n",
       "21         21  [issu, made, take, rais, debat, clear, respons...    29.196615\n",
       "22         22  [wale, chancellor, welsh, harm, boost, cardiff...    28.459207\n",
       "23         23  [role, play, march, peac, exempt, round, prosp...    27.996586\n",
       "24         24  [foreign, leed, soon, franc, abroad, accus, sa...    21.636693\n",
       "25         25  [protect, regul, standard, environ, framework,...    19.508228\n",
       "26         26  [busi, cost, money, financi, scheme, pension, ...    19.389229\n",
       "27         27  [forc, humanitarian, allparti, spread, conflic...    18.509858\n",
       "28         28  [train, east, royal, instrument, line, network...     9.564348\n",
       "29         29  [servic, famili, care, health, need, social, w...     6.775415\n",
       "30         30  [local, fund, author, commun, despit, delay, a...     5.037258"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dataframe for the topics, including the index, the keywords, and the mass probility\n",
    "for j in range(K):\n",
    "    df_topic[j].append(topic_ranks[j][1])\n",
    "df_topic = pd.DataFrame(df_topic, columns=['topic_idx', 'topic_stems', 'probs'])\n",
    "df_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx_doc</th>\n",
       "      <th>idx_topic</th>\n",
       "      <th>prob</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.154041</td>\n",
       "      <td>Con</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.156145</td>\n",
       "      <td>Con</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0.177846</td>\n",
       "      <td>Con</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.243466</td>\n",
       "      <td>Lab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>0.151203</td>\n",
       "      <td>Con</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx_doc  idx_topic      prob source\n",
       "0        0          0  0.154041    Con\n",
       "1        0          5  0.156145    Con\n",
       "2        0         15  0.177846    Con\n",
       "3        1          2  0.243466    Lab\n",
       "4        2         15  0.151203    Con"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# figure out the the probability that each document is associated with each topic\n",
    "# only keep those pairs with prob>=threshold (0.15 here)\n",
    "def get_doc2topics(ldamodel, corpus, sources,threshold=0.15):\n",
    "    data = []\n",
    "    for idx_doc, rows in enumerate(ldamodel[corpus]):\n",
    "        for j, (idx_topic, prob) in enumerate(rows[0]):\n",
    "            if prob < 0.15:\n",
    "                continue\n",
    "            data.append([idx_doc, idx_topic, prob, sources[idx_doc]])\n",
    "    df = pd.DataFrame(data, columns=['idx_doc', 'idx_topic', 'prob', 'source'])\n",
    "    return df\n",
    "\n",
    "df_doc_topic_all = get_doc2topics(lda_model, corpus, sources, 1)\n",
    "df_doc_topic = get_doc2topics(lda_model, corpus, sources)\n",
    "df_doc_topic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16844, (9148,))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_doc_topic.shape[0], df_doc_topic['idx_doc'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({15: 5128,\n",
       "         5: 4496,\n",
       "         21: 3052,\n",
       "         0: 3044,\n",
       "         29: 715,\n",
       "         16: 214,\n",
       "         18: 113,\n",
       "         13: 21,\n",
       "         26: 12,\n",
       "         9: 10,\n",
       "         2: 5,\n",
       "         3: 5,\n",
       "         20: 5,\n",
       "         6: 4,\n",
       "         7: 4,\n",
       "         12: 3,\n",
       "         17: 3,\n",
       "         11: 3,\n",
       "         14: 2,\n",
       "         10: 1,\n",
       "         30: 1,\n",
       "         24: 1,\n",
       "         19: 1,\n",
       "         27: 1})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# count the # documents associated with each topic\n",
    "Counter(df_doc_topic['idx_topic'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a validation set for finetuning the language model\n",
    "# articles that is not assigned to any topic (prob < 0.15) are in this set\n",
    "df.insert(0, 'idx', np.arange(df.shape[0]))\n",
    "idxes_doc_val = set(df['idx'].unique().tolist()) - set(df_doc_topic['idx_doc'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data\n",
    "df_ntopics_coh.to_excel('coh_values.xlsx', index=False)\n",
    "df_doc_topic.to_csv('df_doc_topic.csv', index=False)\n",
    "df_topic.to_csv('df_topics.csv', index=False)\n",
    "pickle.dump(topic_ranks, open('topic_ranks.pkl', 'wb'))\n",
    "pickle.dump(lda_model, open('lda_model.pkl', 'wb'))\n",
    "pickle.dump((lda_models, coh_values, max_idx), open('lda_models.pkl', 'wb'))\n",
    "pickle.dump(idxes_doc_val, open('idxes_val.pkl', 'wb'))\n",
    "# pickle.dump(idxes_doc_val2, open('idxes_val2.pkl', 'wb'))\n",
    "pickle.dump(lda_model.show_topics(num_topics=-1, num_words=10, formatted=False), \n",
    "            open('topics.pkl', 'wb'))\n",
    "#pickle.dump(topic_labels, open('topic_labels.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Doings partisanship learning directly with GloVe**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plutôt ré-entrainer GloVe directement sur les speeches polarisés sélectionnés. Refaire les poids. \n",
    "- Puis faire la technique de masque de l'articles et calculer les embeddings des topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training partisanship learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>year</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>party</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>keywords</th>\n",
       "      <th>agenda</th>\n",
       "      <th>url</th>\n",
       "      <th>cos axe</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>level_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30079</th>\n",
       "      <td>513548.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>Damian Green</td>\n",
       "      <td>Con</td>\n",
       "      <td>[complet, agre, alreadi, mention, enterpris, a...</td>\n",
       "      <td>par</td>\n",
       "      <td>[jobs]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.069410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30080</th>\n",
       "      <td>513561.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>Debbie Abrahams</td>\n",
       "      <td>Lab</td>\n",
       "      <td>[happi, everyon, resolut, foundat, data, show,...</td>\n",
       "      <td>par</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.075862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30081</th>\n",
       "      <td>513567.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>Amanda Milling</td>\n",
       "      <td>Con</td>\n",
       "      <td>[also, wish, happi, thank, answer, famili, res...</td>\n",
       "      <td>par</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.409124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30082</th>\n",
       "      <td>513572.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>Penny Mordaunt</td>\n",
       "      <td>Con</td>\n",
       "      <td>[consciou, need, children, peopl, particular, ...</td>\n",
       "      <td>par</td>\n",
       "      <td>[teams]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.285080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30087</th>\n",
       "      <td>513602.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>Ian Blackford</td>\n",
       "      <td>SNP</td>\n",
       "      <td>[happi, everyon, particularli, waspi, women, b...</td>\n",
       "      <td>par</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8218</th>\n",
       "      <td>584519.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>George Howarth</td>\n",
       "      <td>Lab</td>\n",
       "      <td>[right, absolut, right, anoth, complic, factor...</td>\n",
       "      <td>par</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.038827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8220</th>\n",
       "      <td>584536.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>Ruth Cadbury</td>\n",
       "      <td>Lab</td>\n",
       "      <td>[concur, comment, absolut, right, mere, word, ...</td>\n",
       "      <td>par</td>\n",
       "      <td>[jobs]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.139995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8227</th>\n",
       "      <td>584554.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>John Healey</td>\n",
       "      <td>Lab</td>\n",
       "      <td>[certainli, appli, case, good, point, remain, ...</td>\n",
       "      <td>par</td>\n",
       "      <td>[cook, meta, nest, brin]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.296102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8229</th>\n",
       "      <td>584557.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>Alok Sharma</td>\n",
       "      <td>Con</td>\n",
       "      <td>[come, talk, work, commiss, also, heard, consu...</td>\n",
       "      <td>par</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.283878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8231</th>\n",
       "      <td>584563.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>Alok Sharma</td>\n",
       "      <td>Con</td>\n",
       "      <td>[make, interest, suggest, take, away, come, ba...</td>\n",
       "      <td>par</td>\n",
       "      <td>[cook, nest, brin]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.342870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11124 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            index    year          Speaker party  \\\n",
       "level_0                                            \n",
       "30079    513548.0  2017.0     Damian Green   Con   \n",
       "30080    513561.0  2017.0  Debbie Abrahams   Lab   \n",
       "30081    513567.0  2017.0   Amanda Milling   Con   \n",
       "30082    513572.0  2017.0   Penny Mordaunt   Con   \n",
       "30087    513602.0  2017.0    Ian Blackford   SNP   \n",
       "...           ...     ...              ...   ...   \n",
       "8218     584519.0  2017.0   George Howarth   Lab   \n",
       "8220     584536.0  2017.0     Ruth Cadbury   Lab   \n",
       "8227     584554.0  2017.0      John Healey   Lab   \n",
       "8229     584557.0  2017.0      Alok Sharma   Con   \n",
       "8231     584563.0  2017.0      Alok Sharma   Con   \n",
       "\n",
       "                                                      text source  \\\n",
       "level_0                                                             \n",
       "30079    [complet, agre, alreadi, mention, enterpris, a...    par   \n",
       "30080    [happi, everyon, resolut, foundat, data, show,...    par   \n",
       "30081    [also, wish, happi, thank, answer, famili, res...    par   \n",
       "30082    [consciou, need, children, peopl, particular, ...    par   \n",
       "30087    [happi, everyon, particularli, waspi, women, b...    par   \n",
       "...                                                    ...    ...   \n",
       "8218     [right, absolut, right, anoth, complic, factor...    par   \n",
       "8220     [concur, comment, absolut, right, mere, word, ...    par   \n",
       "8227     [certainli, appli, case, good, point, remain, ...    par   \n",
       "8229     [come, talk, work, commiss, also, heard, consu...    par   \n",
       "8231     [make, interest, suggest, take, away, come, ba...    par   \n",
       "\n",
       "                         keywords agenda  url   cos axe  \n",
       "level_0                                                  \n",
       "30079                      [jobs]     []  NaN  0.069410  \n",
       "30080                          []     []  NaN  0.075862  \n",
       "30081                          []     []  NaN -0.409124  \n",
       "30082                     [teams]     []  NaN -0.285080  \n",
       "30087                          []     []  NaN  0.062566  \n",
       "...                           ...    ...  ...       ...  \n",
       "8218                           []     []  NaN  0.038827  \n",
       "8220                       [jobs]     []  NaN  0.139995  \n",
       "8227     [cook, meta, nest, brin]     []  NaN -0.296102  \n",
       "8229                           []     []  NaN -0.283878  \n",
       "8231           [cook, nest, brin]     []  NaN -0.342870  \n",
       "\n",
       "[11124 rows x 10 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mittens import GloVe \n",
    "from scipy import sparse\n",
    "import json\n",
    "import ast\n",
    "from mittens import Mittens\n",
    "\n",
    "from src.GloVe.coocc_functs import *\n",
    "from src.GloVe.training_functs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for sentence in df['text']:\n",
    "    l+=sentence\n",
    "\n",
    "vocab = list(set(l))\n",
    "word2idx = {v: i for i, v in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dans inter_coocc\n",
      "['also', 'wish', 'happi', 'thank', 'answer', 'famili', 'resourc', 'survey', 'publish', 'last', 'show', 'nearli', 'disabl', 'childrena', 'increas', 'past', 'outlin', 'measur', 'implement', 'take', 'account', 'increas', 'children', 'access', 'support', 'specialist', 'equip', 'requir']\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "items = [(j,t) for j,t in enumerate(df['text'])]\n",
    "\n",
    "coocc = inter_coocc(items, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove2dict(glove_filename):\n",
    "    ''' transforms a txt file of embeddings into a dictionary\n",
    "    Parameters:\n",
    "    -----------\n",
    "    glove_filename : embeddings txt file\n",
    "    '''\n",
    "    with open(glove_filename) as f:\n",
    "        reader = csv.reader(f, delimiter=' ', quoting=csv.QUOTE_NONE)\n",
    "        words = []\n",
    "        mats = []\n",
    "        for line in reader :\n",
    "            if len(clean(line[0], gram='unigram'))>0:\n",
    "                words.append(clean(line[0], gram='unigram')[0])\n",
    "                mats.append(np.array(list(map(float, line[1:]))))\n",
    "    embed = {words[i]: mats[i] for i in range(len(words))}\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_embedding = glove2dict('data/embeddings/embeddings_201'+str(i)+'.txt')\n",
    "coocc  = coocc.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 40: error 88480.24747"
     ]
    }
   ],
   "source": [
    "mittens_model = Mittens(n=50, max_iter=40)\n",
    "    \n",
    "new_embeddings = mittens_model.fit(\n",
    "    coocc,\n",
    "    vocab=vocab,\n",
    "    initial_embedding_dict= original_embedding)\n",
    "    \n",
    "a = np.array(vocab)\n",
    "b = new_embeddings\n",
    "c = np.column_stack((a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(c).to_csv('notebooks/embeddings polarization/re_trained_embeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.array(pd.read_csv('notebooks/embeddings polarization/re_trained_embeddings.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LDA filter the new embeddings and compute polarization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>index</th>\n",
       "      <th>year</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>party</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>keywords</th>\n",
       "      <th>agenda</th>\n",
       "      <th>url</th>\n",
       "      <th>cos axe</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>level_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30081</th>\n",
       "      <td>0</td>\n",
       "      <td>513567.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>Amanda Milling</td>\n",
       "      <td>Con</td>\n",
       "      <td>[also, wish, happi, thank, answer, famili, res...</td>\n",
       "      <td>par</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.409124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30082</th>\n",
       "      <td>1</td>\n",
       "      <td>513572.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>Penny Mordaunt</td>\n",
       "      <td>Con</td>\n",
       "      <td>[consciou, need, children, peopl, particular, ...</td>\n",
       "      <td>par</td>\n",
       "      <td>[teams]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.285080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30090</th>\n",
       "      <td>2</td>\n",
       "      <td>513634.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>Damian Green</td>\n",
       "      <td>Con</td>\n",
       "      <td>[agre, issu, precis, matthew, review, investig...</td>\n",
       "      <td>par</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.249888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30092</th>\n",
       "      <td>3</td>\n",
       "      <td>513644.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>Penny Mordaunt</td>\n",
       "      <td>Con</td>\n",
       "      <td>[invest, signific, resourc, includ, increas, c...</td>\n",
       "      <td>par</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.292079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30093</th>\n",
       "      <td>4</td>\n",
       "      <td>513647.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>Luciana Berger</td>\n",
       "      <td>Lab</td>\n",
       "      <td>[five, forward, view, mental, health, publish,...</td>\n",
       "      <td>par</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.432286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8212</th>\n",
       "      <td>5557</td>\n",
       "      <td>584505.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>Jim Fitzpatrick</td>\n",
       "      <td>Lab</td>\n",
       "      <td>[finger, point, look, reassur, leasehold, cove...</td>\n",
       "      <td>par</td>\n",
       "      <td>[page]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.256374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8214</th>\n",
       "      <td>5558</td>\n",
       "      <td>584512.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>William Wragg</td>\n",
       "      <td>Con</td>\n",
       "      <td>[surpris, agenc, charg, ground, rent, sale, wo...</td>\n",
       "      <td>par</td>\n",
       "      <td>[excel]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.246312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8227</th>\n",
       "      <td>5559</td>\n",
       "      <td>584554.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>John Healey</td>\n",
       "      <td>Lab</td>\n",
       "      <td>[certainli, appli, case, good, point, remain, ...</td>\n",
       "      <td>par</td>\n",
       "      <td>[cook, meta, nest, brin]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.296102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8229</th>\n",
       "      <td>5560</td>\n",
       "      <td>584557.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>Alok Sharma</td>\n",
       "      <td>Con</td>\n",
       "      <td>[come, talk, work, commiss, also, heard, consu...</td>\n",
       "      <td>par</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.283878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8231</th>\n",
       "      <td>5561</td>\n",
       "      <td>584563.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>Alok Sharma</td>\n",
       "      <td>Con</td>\n",
       "      <td>[make, interest, suggest, take, away, come, ba...</td>\n",
       "      <td>par</td>\n",
       "      <td>[cook, nest, brin]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.342870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5562 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          idx     index    year          Speaker party  \\\n",
       "level_0                                                  \n",
       "30081       0  513567.0  2017.0   Amanda Milling   Con   \n",
       "30082       1  513572.0  2017.0   Penny Mordaunt   Con   \n",
       "30090       2  513634.0  2017.0     Damian Green   Con   \n",
       "30092       3  513644.0  2017.0   Penny Mordaunt   Con   \n",
       "30093       4  513647.0  2017.0   Luciana Berger   Lab   \n",
       "...       ...       ...     ...              ...   ...   \n",
       "8212     5557  584505.0  2017.0  Jim Fitzpatrick   Lab   \n",
       "8214     5558  584512.0  2017.0    William Wragg   Con   \n",
       "8227     5559  584554.0  2017.0      John Healey   Lab   \n",
       "8229     5560  584557.0  2017.0      Alok Sharma   Con   \n",
       "8231     5561  584563.0  2017.0      Alok Sharma   Con   \n",
       "\n",
       "                                                      text source  \\\n",
       "level_0                                                             \n",
       "30081    [also, wish, happi, thank, answer, famili, res...    par   \n",
       "30082    [consciou, need, children, peopl, particular, ...    par   \n",
       "30090    [agre, issu, precis, matthew, review, investig...    par   \n",
       "30092    [invest, signific, resourc, includ, increas, c...    par   \n",
       "30093    [five, forward, view, mental, health, publish,...    par   \n",
       "...                                                    ...    ...   \n",
       "8212     [finger, point, look, reassur, leasehold, cove...    par   \n",
       "8214     [surpris, agenc, charg, ground, rent, sale, wo...    par   \n",
       "8227     [certainli, appli, case, good, point, remain, ...    par   \n",
       "8229     [come, talk, work, commiss, also, heard, consu...    par   \n",
       "8231     [make, interest, suggest, take, away, come, ba...    par   \n",
       "\n",
       "                         keywords agenda  url   cos axe  \n",
       "level_0                                                  \n",
       "30081                          []     []  NaN -0.409124  \n",
       "30082                     [teams]     []  NaN -0.285080  \n",
       "30090                          []     []  NaN -0.249888  \n",
       "30092                          []     []  NaN -0.292079  \n",
       "30093                          []     []  NaN -0.432286  \n",
       "...                           ...    ...  ...       ...  \n",
       "8212                       [page]     []  NaN -0.256374  \n",
       "8214                      [excel]     []  NaN -0.246312  \n",
       "8227     [cook, meta, nest, brin]     []  NaN -0.296102  \n",
       "8229                           []     []  NaN -0.283878  \n",
       "8231           [cook, nest, brin]     []  NaN -0.342870  \n",
       "\n",
       "[5562 rows x 11 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_keywords_in_topic = 50\n",
    "number_of_topics = 5\n",
    "number_of_documents_for_one_side = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarized_embeddings = dict([(word[0], embedding) for word, embedding in zip(c[:, :1], c[:, 1:])], dtypes='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_doc_topic_left = df_doc_topic[df_doc_topic['source'] == 'Lab']\n",
    "df_doc_topic_right = df_doc_topic[df_doc_topic['source'] == 'Con']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_id = 21\n",
    "\n",
    "working_df_left = df_doc_topic_left[df_doc_topic_left['idx_topic'] == topic_id]\n",
    "working_df_right = df_doc_topic_right[df_doc_topic_right['idx_topic'] == topic_id]\n",
    "\n",
    "working_df_right = working_df_right.sort_values('prob', ascending=False)\n",
    "working_df_left = working_df_left.sort_values('prob', ascending=False)\n",
    "\n",
    "working_df_right = working_df_right.head(number_of_documents_for_one_side)\n",
    "working_df_left = working_df_left.head(number_of_documents_for_one_side)\n",
    "\n",
    "working_df = pd.concat([working_df_left, working_df_right])\n",
    "df = df.rename(columns={\"idx\": \"idx_doc\"})\n",
    "merged_df = pd.merge(working_df, df, on=['idx_doc'])[['idx_doc', 'idx_topic', 'prob', 'source_x', 'text']]\n",
    "merged_df.set_index('idx_doc', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights_dict = dict(lda_models[K].show_topics(num_topics=-1, num_words=20, formatted=False)[topic_id][1][:number_of_keywords_in_topic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_keywords_from_topic_and_compute_weighted_embeddings(topic_id, number_of_keywords_in_topic, text:list, polarized_embeddings):\n",
    "\n",
    "    list_of_keywords = [a for a,b in lda_models[K].show_topics(num_topics=-1, num_words=20, formatted=False)[topic_id][1][:number_of_keywords_in_topic]]\n",
    "    filtered_text = [word for word in text if word in list_of_keywords]\n",
    "\n",
    "    new_weights_dict = dict(lda_models[K].show_topics(num_topics=-1, num_words=20, formatted=False)[topic_id][1][:number_of_keywords_in_topic])\n",
    "\n",
    "    embed = np.zeros(50, dtype='float')\n",
    "\n",
    "    for word in filtered_text:\n",
    "\n",
    "        a = np.array(new_weights_dict[word])\n",
    "        b = np.array(polarized_embeddings[word])\n",
    "\n",
    "        embed = embed + a*b\n",
    "\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['weighted_topic_document_embeddings_1'] = merged_df['text'].apply(lambda x: filter_keywords_from_topic_and_compute_weighted_embeddings(topic_id, number_of_keywords_in_topic, x, polarized_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['weighted_topic_document_embeddings_2'] = np.multiply(np.array(merged_df['prob']), np.array(merged_df['weighted_topic_document_embeddings_1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_df_left, final_df_right = merged_df[merged_df['source_x'] == 'Lab'], merged_df[merged_df['source_x'] == 'Con']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_probability_per_topic(lda_model, corpus):\n",
    "    # Initialize a structure to hold sums of document probabilities per topic\n",
    "    topic_doc_contribution = {i: {} for i in range(lda_model.num_topics)}\n",
    "    \n",
    "    # Aggregate probabilities of topics within each document\n",
    "    for doc_id, doc_bow in enumerate(corpus):\n",
    "        doc_topics = lda_model.get_document_topics(doc_bow, minimum_probability=0)\n",
    "        for topic_id, prob in doc_topics:\n",
    "            # Initialize the nested dictionary if the doc_id doesn't exist for the topic_id\n",
    "            if doc_id not in topic_doc_contribution[topic_id]:\n",
    "                topic_doc_contribution[topic_id][doc_id] = 0\n",
    "            topic_doc_contribution[topic_id][doc_id] += prob\n",
    "    \n",
    "    # Normalize these sums to estimate P(d|t) for each topic\n",
    "    for topic_id in topic_doc_contribution:\n",
    "        total_contribution = sum(topic_doc_contribution[topic_id].values())\n",
    "        for doc_id in topic_doc_contribution[topic_id]:\n",
    "            topic_doc_contribution[topic_id][doc_id] /= total_contribution\n",
    "    \n",
    "    return topic_doc_contribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(d/t) = P(t/d)*P(d)/P(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_probability_per_topic = document_probability_per_topic(lda_model, corpus)\n",
    "merged_df['final_weights'] = np.multiply(np.array([document_probability_per_topic[topic_id][idx] for idx in list(merged_df.index)]), merged_df['weighted_topic_document_embeddings_2'])\n",
    "merged_df = merged_df[['source_x', 'final_weights']]\n",
    "final_df = merged_df.groupby(by='source_x').mean()\n",
    "first_embedding, second_embedding = list(final_df['final_weights']) \n",
    "cosine = np.dot(first_embedding, second_embedding.T) / (norm(first_embedding) * norm(second_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9996148371649755"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BERT Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Start from GloVe embeddings\n",
    "- Select polarized speeches close to poles with distribution and percentiles\n",
    "- Essayer d'optimiser la LDA avec ce choix de percentiles\n",
    "- This is our two parties corpuses, train BERT to predict right party"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a mask for each document. This mask indicates how much each token of the document will contribute to the document-contextualized topic embedding (the weights). The length of the mask is equal to the # of tokens in the dcoument (tokenized by BERT). Each element in the mask indicates the weight of the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'notebooks/embeddings polarization/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/alexandrequeant/Desktop/Travail TSE/notebooks/embeddings polarization'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_processed_bert = data\n",
    "pickle.dump(text_processed_bert, open('texts_processed_bert.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = pickle.load(open('lda_model.pkl', 'rb'))\n",
    "df_doc_topic = pd.read_csv('df_doc_topic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the documents by BERT\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "texts_bert = pickle.load(open('texts_processed_bert.pkl', 'rb'))\n",
    "text_encodings = tokenizer(pd.Series(texts_bert).apply(lambda x: ' '.join(x)).tolist(), padding=True, \n",
    "                           truncation=True)['input_ids']\n",
    "text_encodings = pd.Series(text_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_in_list(list1, list2):\n",
    "    '''\n",
    "    search the indices of the topic keywords in the tokenized document -- a list of tokens\n",
    "    '''\n",
    "    idxes = []\n",
    "    for i in range(len(list2)):\n",
    "        if list1[0] == list2[i]:\n",
    "            if (i + len(list1) <= len(list2)) and list2[i:i+len(list1)] == list1:\n",
    "                idxes += list(range(i, i+len(list1)))\n",
    "    return idxes\n",
    "\n",
    "topic_masks = np.zeros((text_encodings.shape[0], 512))  # (n_docs, 512)\n",
    "for topic in lda_model.show_topics(num_topics=-1, num_words=50, formatted=False):\n",
    "    idx = topic[0]\n",
    "    topic_stems = [each[0] for each in topic[1]]\n",
    "    stem_probs = [each[1] for each in topic[1]]\n",
    "    stem_probs = np.array(stem_probs)\n",
    "    stem_probs /= stem_probs.sum()  # normalize the weights of the top-n keywords\n",
    "    stem_encodings = tokenizer(topic_stems, truncation=True)['input_ids'] # encode topic keywords using BERT\n",
    "    doc_idxes = df_doc_topic[df_doc_topic['idx_topic'] == idx]['idx_doc'].to_list() # find the documents associated with the topic\n",
    "    doc_encodings = text_encodings[doc_idxes]\n",
    "    for doc_idx in doc_idxes:\n",
    "        doc_encoding = doc_encodings[doc_idx]\n",
    "        topic_mask = topic_masks[doc_idx]\n",
    "        for stem_input_ids, stem_prob in zip(stem_encodings, stem_probs):\n",
    "            idxes = search_in_list(stem_input_ids[1:-1], doc_encoding) # search each keyword in the document\n",
    "            if idxes:\n",
    "                # if found multiple occurrences of the keywords, \n",
    "                # then the weight of each occurrence will be devalued\n",
    "                topic_mask[idxes] += stem_prob / len(idxes) \n",
    "        if topic_mask.mean() > 0:\n",
    "            topic_mask /= topic_mask.sum()   # normalize the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(topic_masks.tolist(), open('topic_masks.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.29404683, 0.01292744, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.02623017, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_masks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
